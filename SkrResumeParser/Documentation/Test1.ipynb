{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# First Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each Test is divided by their level . Each level contain their job title test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this test we are going to train feed forward neural network with two hidden layer with each job title data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first job title is web development of level 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make data preprocessing templete of level 0 \n",
    "which will be used by all job title test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "their are following steps for data processing\n",
    "    1. find all vocablory in files\n",
    "    2. divide all vocablory in two parts\n",
    "        a. known vocab => which have word2vec in word embeddided file\n",
    "        b. unknown vocab => which does not have word2vec \n",
    "    3. find all known vvocab vector and generate all unknown voacb vector and save in cpickle file\n",
    "    4. take each sentence from training file and make sentence vector using word2vec file and save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "import cPickle as pic\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'      # this is used when you want to run on cpu not gpu\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(file_name):\n",
    "    lexicon = []\n",
    "    with io.open(file_name, 'r', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            #all_words = word_tokenize(l)\n",
    "            lexicon += list(all_words)\n",
    "    lexicon = [(lemmatizer.lemmatize(i)).encode('utf8').lower() for i in lexicon]\n",
    "    lexicon = sorted(set(lexicon))\n",
    "\n",
    "    return lexicon\n",
    "\n",
    "def get_unknown_vocab(fname, words):\n",
    "    f2 = io.open(fname, 'rb').read()\n",
    "    vocab = f2.splitlines()\n",
    "    known_vocab = []\n",
    "    unknown_vocab = []\n",
    "    for word in words:\n",
    "        if word in vocab:\n",
    "            known_vocab.append(word)\n",
    "        else:\n",
    "            unknown_vocab.append(word)\n",
    "    return known_vocab, unknown_vocab\n",
    "\n",
    "def get_wiki_glove_vector(fname, words):\n",
    "    vectors = {}\n",
    "    W = []\n",
    "    vocab = {}\n",
    "    ivocab = {}\n",
    "    f1 = open(fname, 'rb').read()\n",
    "    for line in f1.splitlines():\n",
    "        temp = line.split()\n",
    "        vectors[temp[0]] = map(float, temp[1:])\n",
    "\n",
    "    vocab_size = len(words)\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        W.append(vectors[words[i]])\n",
    "        vocab[words[i]] = i\n",
    "        ivocab[i] = words[i]\n",
    "    W = np.array(W)\n",
    "    # normalize each word vector to unit variance\n",
    "    #print W[0:2], W[-1:-3]\n",
    "    W_norm = np.zeros(W.shape)\n",
    "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "    W_norm = (W.T / d).T\n",
    "\n",
    "    return W_norm, vocab, ivocab\n",
    "\n",
    "\n",
    "def get_unknown_vec(words, word2vec, vocab, ivocab):\n",
    "    old_size = len(vocab)\n",
    "    word2vec = list(word2vec)\n",
    "    for i in range(len(words)):\n",
    "        word2vec.append(np.random.uniform(-0.25, 0.25, len(word2vec[0])))\n",
    "        vocab[words[i]] = i+old_size\n",
    "        ivocab[i+old_size] = words[i]\n",
    "    return word2vec, vocab, ivocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-134db52e8df6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mword2vec_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/sumit/PycharmProjects/ResumeParser/Data/resume_segments/Test/web_word2vec.p\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdirlist_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/sumit/PycharmProjects/ResumeParser/Data/resume_segments/Level0/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdirlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirlist_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# only give files not folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#dirlist = os.listdir(dirlist_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "glove_file = '/home/sumit/stanford-nlp/glove/wiki_GVector/glove.6B.100d.txt'\n",
    "glove_vocab_file = '/home/sumit/stanford-nlp/glove/wiki_GVector/vocab.txt'\n",
    "word2vec_file = \"/home/sumit/PycharmProjects/ResumeParser/Data/resume_segments/Test/web_word2vec.p\"\n",
    "dirlist_path = '/home/sumit/PycharmProjects/ResumeParser/Data/resume_segments/Level0/'\n",
    "dirlist = os.walk(dirlist_path).next()[2] # only give files not folders\n",
    "#dirlist = os.listdir(dirlist_path)\n",
    "\n",
    "if not os.path.isfile(word2vec_file):\n",
    "    # get vocablory of all file\n",
    "    vocab = []\n",
    "\n",
    "    for files in dirlist:\n",
    "        vocab += get_vocab(dirlist_path+files)\n",
    "    vocab = sorted(set(vocab))\n",
    "    print(\"vocab length: \", len(vocab))\n",
    "    # get the glove vector\n",
    "    known_vocab, unknown_vocab = get_unknown_vocab(glove_vocab_file, vocab)\n",
    "    print(\"known_vocab length: \", len(known_vocab))\n",
    "    print(\"unknown_vocab length: \", len(unknown_vocab))\n",
    "    Word2vec, vocab, ivocab = get_wiki_glove_vector(glove_file, known_vocab)\n",
    "    Word2vec, vocab, ivocab = get_unknown_vec(unknown_vocab,Word2vec, vocab, ivocab)\n",
    "    pic.dump([Word2vec, vocab, ivocab ], open(word2vec_file, 'wb'))\n",
    "    print(\"word2vec created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_sen2vec(fname, word2vec_fname, output):\n",
    "    sen2vec = []\n",
    "    with open(word2vec_fname, 'rb') as f:\n",
    "        word2vec, vocab, ivocab = pic.load(f)\n",
    "\n",
    "    with io.open(fname, 'r', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            temp = np.zeros(len(word2vec[0]), dtype=float)\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            lexicon = list(all_words)\n",
    "            lexicon = [(lemmatizer.lemmatize(i)).encode('utf8').lower() for i in lexicon]\n",
    "            for word in lexicon:\n",
    "                temp += word2vec[vocab[word]]\n",
    "            sen2vec.append([temp, output])\n",
    "    return sen2vec\n",
    "\n",
    "def get_2D_sen2vec(fname, word2vec_fname, output, m):\n",
    "    sen2vec = []\n",
    "    with open(word2vec_fname, 'rb') as f:\n",
    "        word2vec, vocab, ivocab = pic.load(f)\n",
    "\n",
    "    with io.open(fname, 'r', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            temp = np.zeros([m, len(word2vec[0])], dtype=float)\n",
    "            count = 0\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            lexicon = list(all_words)\n",
    "            lexicon = [(lemmatizer.lemmatize(i)).encode('utf8').lower() for i in lexicon]\n",
    "            for word in lexicon:\n",
    "                temp[count] = word2vec[vocab[word]]\n",
    "                count += 1\n",
    "            sen2vec.append([temp, output])\n",
    "    return sen2vec\n",
    "\n",
    "sen2vec_file = \"/home/sumit/PycharmProjects/ResumeParser/Data/resume_segments/Test/web_sen2vec.p\"\n",
    "\n",
    "\n",
    "if not os.path.isfile(sen2vec_file):\n",
    "    # get sentence vector from file\n",
    "    sen2vec = []\n",
    "    count  = 0\n",
    "\n",
    "    for files in dirlist:\n",
    "        output = [0 for i in range(9)]\n",
    "        output[count] = 1\n",
    "        sen2vec +=  get_avg_sen2vec(dirlist_path+files, word2vec_file, output)\n",
    "        count += 1\n",
    "    import random\n",
    "    random.shuffle(sen2vec)\n",
    "    pic.dump(sen2vec, open(sen2vec_file, 'wb'))\n",
    "    print(\"sen2vec created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = io.open(sen2vec_file, 'rb')\n",
    "sen2vec = pic.load(f)\n",
    "f.close()\n",
    "print(len(sen2vec))\n",
    "\n",
    "train_data = np.array(sen2vec[0:int(len(sen2vec)*0.8)])\n",
    "test_data = np.array(sen2vec[int(len(sen2vec)*0.8):])\n",
    "train_x = np.array(list(train_data[:, 0]))\n",
    "train_y = np.array(list(train_data[:, 1]))\n",
    "test_x = np.array(list(test_data[:, 0]))\n",
    "test_y = np.array(list(test_data[:, 1]))\n",
    "print(\"train data length: \", len(train_data))\n",
    "print(\"test data length: \", len(test_data))\n",
    "print(\"structure of train_x\", train_x[0])\n",
    "print(\"structure of train_y\", train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu', input_dim = 100))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'softmax'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(train_x, train_y, batch_size = 30, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print dirlist\n",
    "y_pred = classifier.predict(test_x)\n",
    "\n",
    "\n",
    "correct = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\n",
    "accuracy = np.mean(correct)\n",
    "print(\"Accuracy\", accuracy)\n",
    "print \"pred: \", np.argmax(y_pred, 1)[0:30]\n",
    "print \"real: \", np.argmax(test_y, 1)[0:30]\n",
    "print \"pred: \", np.argmax(y_pred, 1)[30:65]\n",
    "print \"real: \", np.argmax(test_y, 1)[30:65]\n",
    "print \"pred: \", np.argmax(y_pred, 1)[65:100]\n",
    "print \"real: \", np.argmax(test_y, 1)[65:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec_file = \"/home/sumit/PycharmProjects/ResumeParser/Data/resume_segments/Test/web_sen2vec2D.p\"\n",
    "\n",
    "\n",
    "if not os.path.isfile(sen2vec_file):\n",
    "    # get sentence vector from file\n",
    "    sen2vec = []\n",
    "    count  = 0\n",
    "    for files in dirlist:\n",
    "        output = [0 for i in range(9)]\n",
    "        output[count] = 1\n",
    "        sen2vec +=  get_2D_sen2vec(dirlist_path+files, word2vec_file, output, 50)\n",
    "        count += 1\n",
    "    import random\n",
    "    random.shuffle(sen2vec)\n",
    "    pic.dump(sen2vec, open(sen2vec_file, 'wb'))\n",
    "    print(\"sen2vec created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = io.open(sen2vec_file, 'rb')\n",
    "sen2vec = pic.load(f)\n",
    "f.close()\n",
    "print(len(sen2vec))\n",
    "\n",
    "train_data = np.array(sen2vec[0:int(len(sen2vec)*0.8)])\n",
    "test_data = np.array(sen2vec[int(len(sen2vec)*0.8):])\n",
    "train_x = np.array(list(train_data[:, 0]))\n",
    "train_y = np.array(list(train_data[:, 1]))\n",
    "test_x = np.array(list(test_data[:, 0]))\n",
    "test_y = np.array(list(test_data[:, 1]))\n",
    "print(\"train data length: \", len(train_data))\n",
    "print(\"test data length: \", len(test_data))\n",
    "print(\"structure of train_x\", train_x[0])\n",
    "print(\"structure of train_y\", train_y[0])\n",
    "train_x = np.reshape(train_x, [-1, 50, 100, 1])\n",
    "test_x = np.reshape(test_x, [-1, 50, 100, 1])\n",
    "print(train_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "cnn_classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "cnn_classifier.add(Conv2D(50, (4, 100), input_shape = (50, 100, 1), activation = 'relu'))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "cnn_classifier.add(AveragePooling2D(pool_size = (2, 1)))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "cnn_classifier.add(Flatten())\n",
    "\n",
    "# Step 4 - Full connection\n",
    "cnn_classifier.add(Dense(units = 1024, activation = 'relu'))\n",
    "cnn_classifier.add(Dense(units = 512, activation = 'relu'))\n",
    "cnn_classifier.add(Dense(units = 9, activation = 'softmax'))\n",
    "\n",
    "# Compiling the CNN\n",
    "cnn_classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "cnn_classifier.fit(train_x, train_y, batch_size = 30, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print dirlist\n",
    "y_pred = cnn_classifier.predict(test_x)\n",
    "\n",
    "\n",
    "correct = np.equal(np.argmax(y_pred, 1), np.argmax(test_y, 1))\n",
    "accuracy = np.mean(correct)\n",
    "print(\"Accuracy\", accuracy)\n",
    "print \"pred: \", np.argmax(y_pred, 1)[0:30]\n",
    "print \"real: \", np.argmax(test_y, 1)[0:30]\n",
    "print \"pred: \", np.argmax(y_pred, 1)[30:65]\n",
    "print \"real: \", np.argmax(test_y, 1)[30:65]\n",
    "print \"pred: \", np.argmax(y_pred, 1)[65:100]\n",
    "print \"real: \", np.argmax(test_y, 1)[65:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take any resume and predict the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_file = \"/home/sumit/PycharmProjects/ResumeParser/Data/resume_segments/Test/sample.txt\"\n",
    "# get vocablory of all file\n",
    "vocab = get_vocab(resume_file)\n",
    "print(\"vocab length: \", len(vocab))\n",
    "# get the glove vector\n",
    "known_vocab, unknown_vocab = get_unknown_vocab(glove_vocab_file, vocab)\n",
    "print(\"known_vocab length: \", len(known_vocab))\n",
    "print(\"unknown_vocab length: \", len(unknown_vocab))\n",
    "Word2vec, vocab, ivocab = get_wiki_glove_vector(glove_file, known_vocab)\n",
    "Word2vec, vocab, ivocab = get_unknown_vec(unknown_vocab,Word2vec, vocab, ivocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_avg_Testsen2vec(fname, word2vec, vocab, ivocab):\n",
    "    sen2vec = []\n",
    "\n",
    "    with io.open(fname, 'r', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            temp = np.zeros(len(word2vec[0]), dtype=float)\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            lexicon = list(all_words)\n",
    "            lexicon = [(lemmatizer.lemmatize(i)).encode('utf8').lower() for i in lexicon]\n",
    "            for word in lexicon:\n",
    "                temp += word2vec[vocab[word]]\n",
    "            sen2vec.append(temp)\n",
    "    return sen2vec\n",
    "\n",
    "def get_2D_Testsen2vec(fname,  word2vec, vocab, ivocab , m):\n",
    "    sen2vec = []\n",
    "\n",
    "    with io.open(fname, 'r', encoding='cp437') as f:\n",
    "        contents = f.readlines()\n",
    "        for l in contents:\n",
    "            temp = np.zeros([m, len(word2vec[0])], dtype=float)\n",
    "            count = 0\n",
    "            all_words = tokenizer.tokenize(l)\n",
    "            lexicon = list(all_words)\n",
    "            lexicon = [(lemmatizer.lemmatize(i)).encode('utf8').lower() for i in lexicon]\n",
    "            for word in lexicon:\n",
    "                temp[count] = word2vec[vocab[word]]\n",
    "                count += 1\n",
    "            sen2vec.append(temp)\n",
    "    return sen2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec = get_avg_Testsen2vec(resume_file, Word2vec, vocab, ivocab)\n",
    "print dirlist\n",
    "\n",
    "y_pred = classifier.predict(np.array(sen2vec))\n",
    "with open(resume_file, 'r') as f:\n",
    "    content = f.readlines()\n",
    "count = 0\n",
    "for line in content:\n",
    "    print line ,'=>', dirlist[np.argmax(y_pred, 1)[count]][:-4]\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = 85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec = get_2D_Testsen2vec(resume_file, Word2vec, vocab, ivocab, 50)\n",
    "print dirlist\n",
    "sen2vec = np.reshape(np.array(sen2vec), (len(sen2vec), 50, 100, 1))\n",
    "y_pred = cnn_classifier.predict(np.array(sen2vec))\n",
    "with open(resume_file, 'r') as f:\n",
    "    content = f.readlines()\n",
    "count = 0\n",
    "for line in content:\n",
    "    print line ,'=>', dirlist[np.argmax(y_pred, 1)[count]][:-4]\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec = get_avg_Testsen2vec(resume_file, Word2vec, vocab, ivocab)\n",
    "pred = classifier.predict(np.array(sen2vec))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x = StandardScaler()\n",
    "pred = sc_x.fit_transform(pred)\n",
    "index = np.array([[i] for i in range(1, len(sen2vec)+1)])\n",
    "pred = np.append(pred, index, axis=1)\n",
    "\n",
    "print pred[0:5]\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    cluster = KMeans(n_clusters=i)\n",
    "    cluster.fit(pred)\n",
    "    wcss.append(cluster.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.show()\n",
    "for j in range(9):\n",
    "    cluster = KMeans(n_clusters=3)\n",
    "    y_pred = cluster.fit_predict(pred[:, [j, 9]])\n",
    "    print pred[0, [j, 9]]\n",
    "    color = ['red', 'blue', 'green', 'pink', 'black']\n",
    "\n",
    "    for i in range(5):\n",
    "        plt.scatter(pred[y_pred==i, j], pred[y_pred==i, 9], color=color[i], s=100)\n",
    "    #plt.scatter(cluster.cluster_centers_[:,0], cluster.cluster_centers_[:,1], s=300, color='yellow')\n",
    "    plt.show()\n",
    "    print y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2vec = get_avg_Testsen2vec(resume_file, Word2vec, vocab, ivocab)\n",
    "pred = classifier.predict(np.array(sen2vec))\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc_x = StandardScaler()\n",
    "#pred = sc_x.fit_transform(pred)\n",
    "index = np.array([[i] for i in range(1, len(sen2vec)+1)])\n",
    "pred = np.append(pred, index, axis=1)\n",
    "\n",
    "print pred[0:5]\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy as  sch\n",
    "denfrograms = sch.dendrogram(sch.linkage(pred, method='ward'))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.show()\n",
    "for j in range(9):\n",
    "    from sklearn.cluster import AgglomerativeClustering as AC\n",
    "    cluster = AC(n_clusters=3)\n",
    "    y_pred = cluster.fit_predict(pred[:, [j, 9]])\n",
    "    print pred[0, [j, 9]]\n",
    "    color = ['red', 'blue', 'green', 'pink', 'black']\n",
    "\n",
    "    for i in range(5):\n",
    "        plt.scatter(pred[y_pred==i, j], pred[y_pred==i, 9], color=color[i], s=100)\n",
    "    #plt.scatter(cluster.cluster_centers_[:,0], cluster.cluster_centers_[:,1], s=300, color='yellow')\n",
    "    plt.show()\n",
    "    print y_pred\n",
    "print \"full\"\n",
    "cluster = AC(n_clusters=3)\n",
    "y_pred = cluster.fit_predict(pred)\n",
    "print y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy = 77.5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
