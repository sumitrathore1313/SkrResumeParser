{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Planning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### their will be folllowing steps \n",
    "    1. predict name from classifier5\n",
    "    2. predict name and email from old-rp\n",
    "    3. decide final name and email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from VectorProcessing import GramGloveSentenceVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MichellePegler.pdf.txt', 'InnaBarmash.pdf.txt', 'JacobStelman.pdf.txt', 'TT HaswinyDeva.pdf.txt', 'StevenDang.pdf.txt', 'TylerDaluz.pdf.txt', 'DanielMintzLinkedIn.pdf.txt', 'EdwinAmirian.pdf.txt', 'StevenAguiar.pdf.txt', 'BrendanWhiting.pdf.txt', 'TanzimMilkey.pdf.txt', 'VeronicaShea.pdf.txt', 'MichelleGeng.pdf.txt', 'PavelGurevich.pdf.txt', 'MasonBrunnick.pdf.txt', 'AshleyLyle.pdf.txt', 'ErinPellegrino.pdf.txt', 'RosalynLau.pdf.txt', 'TTHaswinyDeva.pdf.txt', 'KatherineErnst.pdf.txt', 'SophiaFeng.pdf.txt', 'AndrewAng.pdf.txt', 'TrippPettigrew-Rolapp.pdf.txt', 'MicahTillman.pdf.txt', 'LeahKovach.pdf.txt', 'OriRatner.pdf.txt', 'IsaacMast2.pdf.txt', 'Shilpika (Pi)Chowdhury.pdf.txt', 'AbbyTMiller.pdf.txt', 'ThomasWasko.pdf.txt', 'TrishaHaut\\xc3\\xa9a.pdf.txt', 'Shilpika(Pi)Chowdhury.pdf.txt', 'IanLee.pdf.txt', 'JohannChristineAlcaraz.pdf.txt', 'SamStoner.pdf.txt', 'RahulNemani.pdf.txt', 'JonnyLeaton.pdf.txt', 'Emilie LimaBurke.pdf.txt', 'NikeOnifade.pdf.txt', 'PatrickS.Lee,PMP.pdf.txt', 'TaylorScutti.pdf.txt', 'EmilieLimaBurke.pdf.txt', 'Pavika (PJ)Buddhari.pdf.txt', 'JazzyEllis.pdf.txt', 'RossKinsman.pdf.txt', 'MadelinWoods.pdf.txt', 'JocelynJackson.pdf.txt', 'Johann ChristineAlcaraz.pdf.txt', 'ThomasGarnett.pdf.txt', 'SethNolan.pdf.txt', 'ChristopherChang.pdf.txt', 'LucasKaiser.pdf.txt', 'IsaacMast.pdf.txt', 'ElisseJean-Pierre.pdf.txt']\n"
     ]
    }
   ],
   "source": [
    "classifier_path = \"/home/sumit/Desktop/project17/ResumeParser/Data/resume_segments/Test/classifier5\"\n",
    "dir_path = \"/home/sumit/Desktop/project17/ResumeParser/Data/resume_samples/txts/samples/\"\n",
    "import os\n",
    "dir_list = os.listdir(dir_path)\n",
    "print dir_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 1500\n"
     ]
    }
   ],
   "source": [
    "gsv = GramGloveSentenceVector(dir_path+dir_list[0],dimension=300, training=False)\n",
    "sen2vec = gsv.get_5gram_sentenceVector()\n",
    "print len(sen2vec), len(sen2vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = load_model(classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_pred = classifier.predict(np.array(sen2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Michelle Pegler', 'Senior Business Intelligence Analyst, Federal Reserve', 'Smartly', 'Smartly', '', 'Page1', '', 'Michelle Pegler', '']\n"
     ]
    }
   ],
   "source": [
    "with open(dir_path+dir_list[0], 'r') as f:\n",
    "    content = f.readlines()\n",
    "count = 0\n",
    "labels = ['basic', 'experience', 'education', 'certificate', 'extra', 'skills', 'projects','summary', 'mimc']\n",
    "basic = []\n",
    "for line in content:\n",
    "    if np.argmax(y_pred, 1)[count] == 0:\n",
    "        basic.append(line.strip())\n",
    "    count += 1\n",
    "print basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "st = StanfordNERTagger('/home/sumit/stanford/stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz', '/home/sumit/stanford/stanford-ner/stanford-ner.jar')\n",
    "\n",
    "def getNER(data):\n",
    "    return st.tag(data.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(u'Michelle', u'PERSON'), (u'Pegler', u'PERSON')], [(u'Senior', u'O'), (u'Business', u'ORGANIZATION'), (u'Intelligence', u'ORGANIZATION'), (u'Analyst,', u'ORGANIZATION'), (u'Federal', u'ORGANIZATION'), (u'Reserve', u'ORGANIZATION')], [(u'Smartly', u'O')], [(u'Smartly', u'O')], [], [(u'Page1', u'O')], [], [(u'Michelle', u'PERSON'), (u'Pegler', u'PERSON')], []]\n",
      "[u'Michelle Pegler', '', '', '', '', '', '', u'Michelle Pegler', '']\n"
     ]
    }
   ],
   "source": [
    "ner_basic = []\n",
    "for line in basic:\n",
    "    ner_basic.append(getNER(line))\n",
    "print ner_basic\n",
    "name = []\n",
    "temp = []\n",
    "for i in range(len(ner_basic)):\n",
    "    for j in range(len(ner_basic[i])):\n",
    "        if ner_basic[i][j][1] == 'PERSON':\n",
    "            temp.append(ner_basic[i][j][0])\n",
    "    if len(temp):\n",
    "        name.append(\" \".join(temp))\n",
    "    temp = []\n",
    "print name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def basic(fname):\n",
    "    url = \"http://127.0.0.1:8000/resume/upload/\"+fname\n",
    "\n",
    "    files = {'file': open('/home/sumit/Desktop/project17/ResumeParser/Data/resume_samples/txts/samples/'+fname, 'rb')}\n",
    "    r = requests.post(url, files=files)\n",
    "\n",
    "    return r.json()['data']['basics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Michelle Pegler', '', '', '', '', '', '', u'Michelle Pegler', '', u'surname firstName', u'surname firstName', u'Michelle Pegler']\n"
     ]
    }
   ],
   "source": [
    "name.append(basic(dir_list[0])['name']['firstName']+' '+basic(dir_list[0])['name']['surname'])\n",
    "print name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
